{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 1: Fast R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast R-CNN is a deep learning solution for object detection, which was proposed by Ross Girshick from Microsoft Research at 2015. He presented the paper titled \"Fast R-CNN\"\\[1\\] on ICCV'15 conference. This paper has already more than 3700 citations.\n",
    "\n",
    "The author has put the source codes of Fast R-CNN on github, but they were written in python and C, using the Caffe framework. Besides, as the author proposed a stronger solution \"Faster R-CNN\" in the same year, the following contributors usually go directly to Faster R-CNN. Thus, you can find a lot of implementations of Faster R-CNN using different languages and frameworks, but very few for Fast R-CNN.\n",
    "\n",
    "That is the reason why we choose Fast R-CNN in the R-CNN series. We plan to implement Fast R-CNN using pytorch, relying on the details given by the paper, to solve our object detection task on drone captured images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: prepare for the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every input image for training, there is another input we need to prepare: the bounding box proposals(also called Region of Interest, RoI) for this image. We need to use a very classical algorithm called Selective Search\\[2\\] to generate enough bounding box proposals.\n",
    "\n",
    "Our setting is: generate 32 bounding box proposals for each image, among which 8 have intersection over union (IoU) overlap with a ground-truth bounding box of at least 0.5. The remaining bounding boxes have IoU with ground-truth in the interval [0.1, 0.5).\n",
    "\n",
    "This is the suggestion from the paper. The only different thing between ours and that of the paper in this step is the number of bounding box proposals, 64 for each image in the paper. The reason is we made a lot of attempt to tune the parameters of Selective Search algorithm, but still cannot get that many bounding box proposals. It is because lots of our images have a small object, there is limited space for proposals. \n",
    "\n",
    "To guarantee that we have enough qualified images to train, we cut the number of bounding box proposals to a half. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import selectivesearch\n",
    "import torch\n",
    "import numpy as np\n",
    "import readimg_new\n",
    "import utils\n",
    "import os\n",
    "import random as rm\n",
    "from datetime import datetime\n",
    "\n",
    "segment = \"segment-random\"\n",
    "train_rp_num = 32   # bounding box proposals for images in training set\n",
    "test_rp_num = 1000  # bounding box proposals for images in testing set\n",
    "first_threshold = 0.1\n",
    "second_threshold = 0.5\n",
    "\n",
    "rm.seed(datetime.now())\n",
    "\n",
    "def get_relaxed_candidates (rps, ious, label):\n",
    "    sorted_rps = [list(z[1]) + [label.item()] if z[0] > 0.5 else list(z[1]) + [0] for z in sorted(zip(ious, rps), reverse=True)]\n",
    "    return sorted_rps[0: train_rp_num]\n",
    "\n",
    "def get_region_proposal (img):\n",
    "    # perform selective search0-\n",
    "    img_lbl, regions1 = selectivesearch.selective_search(img, scale=50, sigma=0.8, min_size=5)\n",
    "    img_lbl, regions2 = selectivesearch.selective_search(img, scale=200, sigma=0.8, min_size=5)\n",
    "    img_lbl, regions3 = selectivesearch.selective_search(img, scale=300, sigma=0.8, min_size=5)\n",
    "    img_lbl, regions4 = selectivesearch.selective_search(img, scale=500, sigma=0.8, min_size=5)\n",
    "    pool = set()\n",
    "    for r in regions1 + regions2 + regions3 + regions4:\n",
    "        # excluding same rectangle (with different segments)\n",
    "        if r['rect'] in pool:\n",
    "            continue\n",
    "        if r['size'] < 100:\n",
    "            continue\n",
    "        if r['rect'][2] > 640 * 3 / 4 or r['rect'][3] > 360 * 3 / 4:\n",
    "            continue\n",
    "        if r['rect'][2] < 5 or r['rect'][3] < 5:\n",
    "            continue\n",
    "        pool.add(r['rect'])\n",
    "    return pool\n",
    "\n",
    "flag = \"test\"\n",
    "\n",
    "train_data, train_label = readimg_new.read_data([segment + \"/\" + flag + \"_data\", segment + \"/\" + flag + \"_label\"])\n",
    "root_path = os.getcwd() + '/'\n",
    "file_names = [z.split('.')[0] for z in readimg_new.file_name(root_path + segment + \"/\" + flag + \"_data\")]\n",
    "completed = [z.split('.')[0] for z in readimg_new.file_name(root_path + segment + \"/region_proposals_train\")] + [z.split('.')[0] for z in readimg_new.file_name(root_path + segment + \"/region_proposals_test\")]\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    if file_names[i] in completed:\n",
    "        continue\n",
    "    img = torch.from_numpy(np.transpose(train_data[i].numpy(), (1, 2, 0)))\n",
    "    pool = list(get_region_proposal(img))\n",
    "    bbox_gt = train_label[i][1:]\n",
    "    label = train_label[i][0]\n",
    "    print(i, \": \", str(len(pool)))\n",
    "    '''\n",
    "    # draw rectangles on the original image\n",
    "    fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(6, 6))\n",
    "    ax.imshow(img)\n",
    "    for x, y, w, h in pool:\n",
    "        ax.add_patch(mpatches.Rectangle(\n",
    "            (x, y), w, h, fill=False, edgecolor='red', linewidth=1))\n",
    "    [x, y, w, h] = bbox_gt\n",
    "    ax.add_patch(mpatches.Rectangle(\n",
    "        (x, y), w, h, fill=False, edgecolor='green', linewidth=1))\n",
    "    plt.show()\n",
    "    '''\n",
    "\n",
    "    candidates_obj = []\n",
    "    candidates_backgd = []\n",
    "    candidates_all = []\n",
    "    ious_obj = []\n",
    "    ious_backgd = []\n",
    "    ious_all = []\n",
    "    for rp in pool:\n",
    "        iou = utils.get_IOU(bbox_gt, rp)\n",
    "        candidates_all.append(rp)\n",
    "        ious_all.append(iou)\n",
    "        if iou >= first_threshold:\n",
    "            if iou > second_threshold:\n",
    "                candidates_obj.append(rp)\n",
    "                ious_obj.append(iou)\n",
    "            else:\n",
    "                candidates_backgd.append(rp)\n",
    "                ious_backgd.append(iou)\n",
    "\n",
    "    if flag == \"train\":\n",
    "        if len(ious_obj) < train_rp_num * 0.25 or len(ious_backgd) < train_rp_num * 0.75:\n",
    "            print(file_names[i], \", obj_num not enough: \", len(ious_obj), \", backgd_num not enough: \", len(ious_backgd))\n",
    "            candidates = get_relaxed_candidates (candidates_all, ious_all, label)\n",
    "        else:\n",
    "            candidates = [list(candidates_obj[l]) + [label.item()] for l in range(int(train_rp_num * 0.25))] + [list(candidates_backgd[l]) + [0] for l in range(int(train_rp_num * 0.75))]\n",
    "\n",
    "        rm.shuffle(candidates)\n",
    "\n",
    "        with open(root_path + segment + \"/region_proposals_train/\" + file_names[i] + \".csv\", 'w', newline='') as outfile:\n",
    "            csvwriter = csv.writer(outfile)\n",
    "            csvwriter.writerow([\"x_lefttop\", \"y_lefttop\", \"width\", \"height\", \"label\"])\n",
    "            for j in range(train_rp_num):\n",
    "                csvwriter.writerow(candidates[j])\n",
    "\n",
    "    elif flag == \"test\":\n",
    "        with open(root_path + segment + \"/region_proposals_test/\" + file_names[i] + \".csv\", 'w', newline='') as outfile:\n",
    "            csvwriter = csv.writer(outfile)\n",
    "            csvwriter.writerow([\"x_lefttop\", \"y_lefttop\", \"width\", \"height\", \"label\"])\n",
    "            for j in range(min(test_rp_num, len(pool))):\n",
    "                if ious_all[j] > 0.5:\n",
    "                    csvwriter.writerow(list(pool[j]) + [label.item(), ious_all[j].item()])\n",
    "                else:\n",
    "                    csvwriter.writerow(list(pool[j]) + [0, ious_all[j].item()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a .csv file containing the information of bounding box proposals, each of (x, y, w, h, label). (x, y) is the coordinates of the left-top corner, w and h are width and height of the box. The label is the object label(number) for those with IoU more than 0.5, and 0(background) for those with IoU in [0.1, 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: build the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/fast_rcnn_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Fast R-CNN network takes as input an entire image and a set of bounding box proposals. The network first processes the whole image with several convolutional(conv) and max pooling layers to produce a conv feature map. Then for each pre-generated bounding box proposal for this image, a corresponding projection from the feature map is extracted, then a RoI pooling layer transform the projection into a fixed-length feature vector. Each feature vector is fed into a sequence of fully connected layers that finally branch into two sibling output layers: one that produces softmax probability estimates over 80 classes plus a background class, another layer that outputs four real-valued numbers (x, y, w, h) for each of the 80 classes.\n",
    "\n",
    "In our project, the Fast R-CNN network is built based on a pre-trained vgg16 network.\n",
    "\n",
    "It undergoes three transformations from the vgg16:\n",
    "(1) the last max pooling layer of vgg16 is replaced by a RoI pooling layer, which transform different size feature map projections into a fix-length feature vector of 7*7. This layer is implemented using torch.nn.AdaptiveMaxPool2d().\n",
    "\n",
    "(2) the network's last fully connected layer and softmax are replaced with the two sibling layers described earlier (a fully connected layer and softmax over 81 categories and category-specific bounding-box regressors). The bounding-box regressor uses the parameterization for regression targets given in paper [3].\n",
    "\n",
    "(3) the network is modified to take two data inputs: a list of images and a list of bounding box proposals for those images.\n",
    "\n",
    "One thing to note is, the RoI pooling layer implementation is not stated clear in the paper. For a original image of size 360*640, after 4 max pooling layer, the feature map is of size 512*22*40, each layer is 16 times smaller than the original image. And we need to extract the projection corresponding to the place of each bounding box proposals, ususlly a small size projection. We found that the projection was easily smaller than size 7*7. Then how to do the RoI pooling? Our way is to first pad the projection with (3, 3, 3, 3), so that we make sure the size is enough for RoI pooling. But no doubt this will slower computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "stride_prod = 16\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class fast_rcnn_net(nn.Module):\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        super(fast_rcnn_net, self).__init__()\n",
    "\n",
    "        # TODO: use vgg16 as ConvNet\n",
    "        self.features = nn.Sequential(\n",
    "            # 0-0 conv layer: 3 * 360 * 640 -> 64 * 360 * 640\n",
    "            nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 0-1 conv layer: 64 * 360 * 640 -> 64 * 360 * 640\n",
    "            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 0 max pooling: 64 * 360 * 640 -> 64 * 180 * 320\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "\n",
    "            # 1-0 conv layer: 64 * 180 * 320 -> 128 * 180 * 320\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 1-1 conv layer: 128 * 180 * 320 -> 128 * 180 * 320\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 1 max pooling: 128 * 180 * 320 -> 128 * 90 * 160\n",
    "            nn.MaxPool2d(kernel_size=2, stride =2, padding=0, dilation=1, ceil_mode=False),\n",
    "\n",
    "            # 2-0 conv layer: 128 * 90 * 160 -> 256 * 90 * 160\n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 2-1 conv layer: 256 * 90 * 160 -> 256 * 90 * 160\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 2-2 conv layer: 256 * 90 * 160 -> 256 * 90 * 160\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 2 max pooling: 256 * 90 * 160 -> 256 * 45 * 80\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "\n",
    "            # 3-0 conv layer: 256 * 45 * 80 -> 512 * 45 * 80\n",
    "            nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU()\n",
    "            \n",
    "        )\n",
    "        \n",
    "        # according to the fast-rcnn paper, it is unnecessary to change the parameters of the first 8 conv layers, thus we freeze these conv layers so the parameters won't be updated during training\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False    # freeze these parameters\n",
    "\n",
    "        # from here on, the parameters will be updated by back-propagation\n",
    "        self.features_unfreeze = nn.Sequential(\n",
    "            # 3-1 conv layer: 512 * 45 * 80 -> 512 * 45 * 80\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 3-2 conv layer: 512 * 45 * 80 -> 512 * 45 * 80\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 3 max pooling: 512 * 45 * 80 -> 512 * 22 * 40\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "\n",
    "            # 4-0 conv layer: 512 * 22 * 40 -> 512 * 22 * 40\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 4-1 conv layer: 512 * 22 * 40 -> 512 * 22 * 40\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 4-2 conv layer: 512 * 22 * 40 -> 512 * 22 * 40\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # TODO: the ROI Pooling layer\n",
    "        # the last max pooling layer is replaced by a ROI pooling layer that is configured by setting H=W=7: 7 * 7 * 512\n",
    "        self.roi_pooling = nn.AdaptiveMaxPool2d((7, 7), return_indices=False)\n",
    "\n",
    "        # TODO: continue the vgg fully connected layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            # 0 fully connected\n",
    "            nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            # 1 fully connected\n",
    "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5)\n",
    "        )\n",
    "\n",
    "        # TODO: two sibling output layer: one that produces softmax probability estimates, another outputs four real-valued numbers for each of the object class\n",
    "        self.class_score_layer = nn.Linear(in_features=4096, out_features=output_size, bias=False)\n",
    "\n",
    "        self.bbox_target_layer = nn.Linear(in_features=4096, out_features=(output_size-1)*4, bias=False)\n",
    "\n",
    "    ## bbox regressor uses the parameterization for regression targets given in paper \"Rich feature hierarchies for accurate object detection and semantic segmentation\"\n",
    "    def bbox_target_to_pred_bbox(self, region_proj, bbox_target):\n",
    "        box = torch.Tensor(region_proj).to(device)\n",
    "\n",
    "        r, c, w, h = box[0], box[1], box[2], box[3]\n",
    "\n",
    "        dr = bbox_target[0::4]\n",
    "        dc = bbox_target[1::4]\n",
    "        dw = bbox_target[2::4]\n",
    "        dh = bbox_target[3::4]\n",
    "\n",
    "        pred_bbox = torch.zeros(bbox_target.size(), dtype=bbox_target.dtype).to(device)\n",
    "\n",
    "        pred_bbox[0::4] = w * dr + r\n",
    "        pred_bbox[1::4] = h * dc + c\n",
    "        pred_bbox[2::4] = w * torch.Tensor(np.exp(dw.detach())).to(device)\n",
    "        pred_bbox[3::4] = h * torch.Tensor(np.exp(dh.detach())).to(device)\n",
    "\n",
    "        for i in range(len(pred_bbox.detach())):\n",
    "            if i % 4 == 0 or i % 4 == 1:\n",
    "                pred_bbox[i] = math.ceil(pred_bbox[i] * stride_prod) - 1\n",
    "            if i % 4 == 2 or i % 4 == 3:\n",
    "                pred_bbox[i] = math.floor(pred_bbox[i] * stride_prod) + 1\n",
    "\n",
    "        return pred_bbox\n",
    "\n",
    "    # two forward function: one forward_feature is following by 32 forward_ouput\n",
    "    # the forward_feature is to extract feature map for a image\n",
    "    # the forward_output is to do the projection and fully connected\n",
    "    def forward_feature (self, x):\n",
    "        feature_maps = self.features(x)\n",
    "        feature_maps = self.features_unfreeze(feature_maps)\n",
    "        return feature_maps\n",
    "\n",
    "    def forward_output (self, x, region_projs):\n",
    "        size = x.detach().size()\n",
    "        output = torch.Tensor(size[0], size[1], 7, 7).to(device)\n",
    "        for idx in range(size[0]):\n",
    "            (r, c, w, h) = (int(z) for z in region_projs[idx])\n",
    "            output[idx] = self.roi_pooling(F.pad(x[idx, :, c: c+h, r: r+w], (3, 3, 3, 3)))\n",
    "        output = self.classifier(output.view(size[0], -1))\n",
    "        clf_scores = self.class_score_layer(output)\n",
    "        clf_scores = F.softmax(clf_scores, dim=1)\n",
    "        bbox_targets = self.bbox_target_layer(output)\n",
    "        bbox_pred = torch.Tensor(bbox_targets.detach().size()).to(device)\n",
    "        for idx in range(len(region_projs)):\n",
    "            bbox_pred[idx] = self.bbox_target_to_pred_bbox(region_projs[idx], bbox_targets[idx])\n",
    "        return clf_scores, bbox_pred\n",
    "\n",
    "# the bounding box proposals need to first transform to projection size.\n",
    "def map_region_proposals_to_feature_map (rps):\n",
    "    rp_projs = []\n",
    "    for rp in rps:\n",
    "        (r1, c1, w, h) = rp  # (r1, c1) is the top-left corner of the region proposal, w is width, h is height\n",
    "        r2, c2 = r1 + w - 1, c1 + h - 1  # (r2, c2) is the bottom-right corner\n",
    "\n",
    "        r1_ = min(math.floor(r1 / stride_prod) + 1, 38)  # max index is 39, but we have to guarantee that the projection has at least width 1\n",
    "        c1_ = min(math.floor(c1 / stride_prod) + 1, 20)  # max index is 21, ...\n",
    "        r2_ = math.ceil(r2 / stride_prod) - 1\n",
    "        c2_ = math.ceil(c2 / stride_prod) - 1\n",
    "        w = max(1.0, r2_-r1_+1)\n",
    "        h = max(1.0, c2_-c1_+1)\n",
    "        rp_projs.append((r1_, c1_, w, h))\n",
    "    return rp_projs\n",
    "\n",
    "# calculate the sibiling losses and combine them into one multi-task loss, averaging between all the batches\n",
    "def smooth_multi_task_loss (clf_scores, clf_gtruth, bbox_pred, bbox_gtruth, bbox_label, lambda_):\n",
    "    loss = torch.zeros(len(clf_gtruth))\n",
    "    for idx in range(len(clf_gtruth)):\n",
    "        loss_cls = torch.Tensor([- math.log(max(clf_scores[idx][int(clf_gtruth[idx].item())], 1e-45))]).squeeze(0) # loss for classification\n",
    "        loss_cls.requires_grad_()\n",
    "        loss_bbox = 0 # loss for bounding box\n",
    "\n",
    "        u = int(bbox_label[idx].item())\n",
    "        if u > 0:\n",
    "            loss_bbox = F.smooth_l1_loss(bbox_pred[idx][(u - 1)*4: u*4], bbox_gtruth[idx].type(torch.float), reduction=\"sum\")\n",
    "        else:\n",
    "            loss_bbox = 0\n",
    "\n",
    "        loss[idx] = loss_cls + lambda_ * loss_bbox\n",
    "\n",
    "    return loss.mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loaded the pre-trained vgg16 parameters of the 1-8 conv layers and the fully connected layers. Then we freeze the 1-8 conv layers. So the training will only change the parameters from the 9th conv layer up.\n",
    "\n",
    "There are two forward function: *forward_feature* and *forward_output*. The forward_feature is to extract feature map for a image, and the forward_output is to do the projection and fully connected.\n",
    "\n",
    "Each forward_feature is followed by 32 forward_output and one backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import utils\n",
    "import time\n",
    "import fast_rcnn_network\n",
    "import readimg_new\n",
    "import os\n",
    "from torchvision import models\n",
    "from fast_rcnn_network import fast_rcnn_net\n",
    "from datetime import datetime\n",
    "\n",
    "root_path = os.getcwd()\n",
    "segment = \"segment2\"\n",
    "\n",
    "model_name = ''.join(str(datetime.now())[11:19].split(':'))\n",
    "print(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# get data\n",
    "# region_proposal: r, c, w, h, label; (r, c) is the axis of left-top corner, label is 0(background) if IOU<50% else is the ground truth label\n",
    "train_data, train_label, train_rps_4_imgs, train_rp_labels_4_imgs = readimg_new.read_data_with_rps(\n",
    "    [segment + \"/train_data\", segment + \"/train_label\", segment + \"/region_proposals_train\"])\n",
    "\n",
    "train_rp_labels_4_imgs = torch.Tensor(train_rp_labels_4_imgs)\n",
    "\n",
    "train_data = train_data.type(torch.float).to(device)\n",
    "train_label = train_label.to(device)\n",
    "\n",
    "# map region proposals to feature maps\n",
    "train_rg_projs_4_imgs = []\n",
    "for rps_4_img in train_rps_4_imgs:\n",
    "    train_rg_projs_4_imgs.append(fast_rcnn_network.map_region_proposals_to_feature_map(rps_4_img))\n",
    "train_rg_projs_4_imgs = torch.Tensor(train_rg_projs_4_imgs)\n",
    "\n",
    "# fast rcnn network\n",
    "output_size = 16\n",
    "our_net = fast_rcnn_net(output_size).to(device)\n",
    "\n",
    "# load the vgg16 pre-trained parameter values\n",
    "pretrained_vgg16 = models.vgg16(pretrained=True)\n",
    "pretrained_dict = pretrained_vgg16.state_dict()\n",
    "\n",
    "# update our network(the vgg16 part) with pre-trained vgg16 parameter values\n",
    "our_net_dict = our_net.state_dict()\n",
    "pretrained_dict = dict({k: v for k, v in pretrained_dict.items() if k in our_net_dict})\n",
    "our_net_dict.update(pretrained_dict)\n",
    "our_net.load_state_dict(our_net_dict)\n",
    "\n",
    "\n",
    "# TODO: train net\n",
    "\n",
    "train_data_num = train_data.size()[0]\n",
    "#test_data_num = test_data.size()[0]\n",
    "pixel_num_of_each_pict = np.prod(train_data[0].size()[-2:])\n",
    "start_time = time.time()\n",
    "epoch_num = 20\n",
    "bs = 2  # number of images in each batch, as mentioned in the paper\n",
    "lr = 0.05\n",
    "roi_size = fast_rcnn_network.roi_size\n",
    "roi_padding = fast_rcnn_network.roi_pad\n",
    "train_region_num = 32  # number of region_projections for each train image\n",
    "\n",
    "print(\"epoch_num=\", epoch_num, \", roi_size=\", roi_size, \", roi_pad=\", roi_padding, \", bs=\", bs, \", lr=\", lr)\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    print(\"epoch: \", epoch)\n",
    "    # learning rate strategy : divide the learning rate by 1.5 every 10 epochs\n",
    "    if epoch % 5 == 0 and epoch > 0:\n",
    "        lr /= 1.1\n",
    "\n",
    "    # create a new optimizer at the beginning of each epoch: give the current learning rate\n",
    "    optimizer = torch.optim.SGD(our_net.parameters(), lr=lr)\n",
    "\n",
    "    shuffled_indices = torch.randperm(train_data_num)\n",
    "\n",
    "    running_loss_train = 0\n",
    "    num_train = 0\n",
    "\n",
    "    for count in range(0, train_data_num, bs):\n",
    "        print(\"count:\", count)\n",
    "        # forward and backward pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = shuffled_indices[count: count + bs]\n",
    "        minibatch_train_data = train_data[indices]\n",
    "        minibatch_train_label = train_label[indices]\n",
    "        minibatch_train_rp_label = train_rp_labels_4_imgs[indices]\n",
    "        minibatch_train_rg_projs = train_rg_projs_4_imgs[indices]\n",
    "        train_inputs = minibatch_train_data\n",
    "\n",
    "        train_inputs.requires_grad_()\n",
    "\n",
    "        feature_maps = our_net.forward_feature(train_inputs)\n",
    "\n",
    "        loss = 0\n",
    "        count_train_loss = 0\n",
    "        for i in range(train_region_num):\n",
    "            region_projs = [minibatch_train_rg_projs[j][i] for j in range(min(bs, len(minibatch_train_rg_projs)))] # the ith region for the jth image\n",
    "            rp_labels = [minibatch_train_rp_label[j][i] for j in range(min(bs, len(minibatch_train_rp_label)))]\n",
    "\n",
    "            clf_scores, bbox_pred = our_net.forward_output(feature_maps, region_projs)\n",
    "            clf_gtruth = [z[0] for z in minibatch_train_label]\n",
    "            bbox_gtruth = [z[1:] for z in minibatch_train_label]\n",
    "            loss += fast_rcnn_network.smooth_multi_task_loss(clf_scores, clf_gtruth, bbox_pred, bbox_gtruth, rp_labels, 1)\n",
    "\n",
    "            running_loss_train += loss.detach().item()\n",
    "            num_train += 1\n",
    "            count_train_loss += loss.detach().item()\n",
    "\n",
    "        print(\"count=\", count, \", total train loss=\", count_train_loss, \", lr=\", lr, \", \", minibatch_train_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    total_loss_train = round(running_loss_train / num_train, 4)\n",
    "    elapsed_time = round(time.time() - start_time, 4)\n",
    "    print(\"epoch=\", epoch, \", time=\", elapsed_time, \", train loss=\", total_loss_train, \", lr=\", lr)\n",
    "\n",
    "# save the model parameters\n",
    "torch.save(our_net.state_dict(), segment + \"/model_params_\" + model_name + \".pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output (running on our server with cuda9):\n",
    "\n",
    "091317\n",
    "\n",
    "torch.Size([400, 3, 360, 640]) #training with 400 images\n",
    "\n",
    "torch.LongTensor\n",
    "\n",
    "torch.Size([400, 5]) #label: (class_label, x, y, w, h)\n",
    "\n",
    "epoch_num= 20 , roi_size= 7 , roi_pad= 0 , bs= 2 , lr= 0.05\n",
    "\n",
    "epoch:  0\n",
    "\n",
    "count= 0 , total train loss= 192488.05795288086 , lr= 0.05 ,  tensor([[  5, 304, 148,  35,  60],\n",
    "        [  5, 300, 146,  36,  62]])\n",
    "        \n",
    "count= 2 , total train loss= 95298.66703796387 , lr= 0.05 ,  tensor([[  4,  77, 107, 178,  81],\n",
    "        [  8, 307, 224, 114,  95]])\n",
    "        \n",
    "count= 4 , total train loss= 80875.30766296387 , lr= 0.05 ,  tensor([[  3, 295, 253, 156,  55],\n",
    "        [ 11, 304, 161,  30,  56]])\n",
    "        \n",
    "count= 6 , total train loss= 81827.7059249878 , lr= 0.05 ,  tensor([[  3, 300, 249, 142,  70],\n",
    "        [  4, 280, 103, 171, 117]])\n",
    "        \n",
    "count= 8 , total train loss= 75715.70306396484 , lr= 0.05 ,  tensor([[ 14, 124, 129,  87,  96],\n",
    "        [  3, 329, 248, 119,  75]])\n",
    "        \n",
    "count= 10 , total train loss= 77361.64921450615 , lr= 0.05 ,  tensor([[  3, 339, 249, 100,  75],\n",
    "        [  8, 277, 151,  98, 103]])\n",
    "        \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can not converge.The loss vibrates up and down 10000.\n",
    "\n",
    "I think there is two possible reason:\n",
    "1. Not enough fine tuning due to the slow training speed. At the beginning, we built a network with a very un-efficient RoI pooling layer, in which we did the projection layer by layer (512 feature map layer). This implementation of RoI pooling layer created a lot of intermediate variables, so it took a long time to complete a backward. We lost a lot of time struggling with this network until we realized the reason. Then we change this layer to the current version, the training time is at least 10x faster. Though, compare with a RoI pooling layer written in C, the speed is still slow, but it's already acceptable.\n",
    "\n",
    "2. Not enough training images due to the bad performance of Selective Search. As mentioned before, due to the small size of objects, there is limited space for bounding box proposals, and we cannot get enough qualified images each with 32 proposals meets our settings. Finally, we actually train the network with 500 images from 15 classes.\n",
    "\n",
    "3. The performance of the network is largely relying on the bounding box proposal input, this is a conclusion by previous learners. And the Selective Search algorithm may not be suitable for our case, so we did not do well in this part and then it affect the entire network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[1\\] Girshick, Ross. \"Fast r-cnn.\" Proceedings of the IEEE international conference on computer vision. 2015.\n",
    "\n",
    "\\[2\\] Uijlings, Jasper RR, et al. \"Selective search for object recognition.\" International journal of computer vision 104.2 (2013): 154-171.\n",
    "\n",
    "\\[3\\] Girshick, Ross, et al. \"Rich feature hierarchies for accurate object detection and semantic segmentation.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
